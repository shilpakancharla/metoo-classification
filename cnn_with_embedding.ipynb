{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "british-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install keras==2.4.0\n",
    "import tensorflow\n",
    "import keras\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Activation, Conv1D, Flatten, MaxPooling1D\n",
    "from tensorflow.python.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "furnished-fruit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wqe3stue) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1803<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/Users/shilpakancharla/Documents/metoo-classification/wandb/run-20210418_135535-wqe3stue/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/Users/shilpakancharla/Documents/metoo-classification/wandb/run-20210418_135535-wqe3stue/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">avid-snowflake-1</strong>: <a href=\"https://wandb.ai/shilpakancharla/metoo-classification/runs/wqe3stue\" target=\"_blank\">https://wandb.ai/shilpakancharla/metoo-classification/runs/wqe3stue</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:wqe3stue). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">worthy-galaxy-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/shilpakancharla/metoo-classification\" target=\"_blank\">https://wandb.ai/shilpakancharla/metoo-classification</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/shilpakancharla/metoo-classification/runs/s61f6r1m\" target=\"_blank\">https://wandb.ai/shilpakancharla/metoo-classification/runs/s61f6r1m</a><br/>\n",
       "                Run data is saved locally in <code>/Users/shilpakancharla/Documents/metoo-classification/wandb/run-20210418_135604-s61f6r1m</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init()\n",
    "config = wandb.config\n",
    "\n",
    "# Set parameters\n",
    "config.vocab_size = 1000\n",
    "config.maxlen = 1000\n",
    "config.batch_size = 32\n",
    "config.embedding_dims = 10\n",
    "config.filters = 16\n",
    "config.kernel_size = 3\n",
    "config.hidden_dims = 250\n",
    "config.epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "associate-dragon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2018, 2017, 2019])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('processed_data/clean_data.csv')\n",
    "df.dropna()\n",
    "delete_row = df[df['Years'] == 0].index\n",
    "df = df.drop(delete_row)\n",
    "df['Years'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "olive-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Tweets with no Stopwords'], df['Years'], test_size = 0.33,\n",
    "                                                   random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "italian-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words = config.vocab_size)\n",
    "tokenizer.fit_on_texts(X_train.astype(str))\n",
    "X_train_tokenized = tokenizer.texts_to_matrix(X_train.astype(str))\n",
    "X_test_tokenized = tokenizer.texts_to_matrix(X_test.astype(str))\n",
    "X_train_seq = sequence.pad_sequences(X_train_tokenized, maxlen = config.maxlen)\n",
    "X_test_seq = sequence.pad_sequences(X_test_tokenized, maxlen = config.maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "commercial-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "y_train_encoded = LabelBinarizer().fit_transform(y_train)\n",
    "y_test_encoded = LabelBinarizer().fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "qualified-government",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.21118492 0.5432297  2.99831792]\n",
      "{0: 1.21118492304933, 1: 0.5432297008596592, 2: 2.9983179213948445}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[2017 2018 2019], y=[2017 2018 2018 ... 2017 2017 2018] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "label_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train.ravel())\n",
    "print(label_weights)\n",
    "label_weights = {i:label_weights[i] for i in range(len(label_weights))} # Create dictionary\n",
    "print(label_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "offensive-franchise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(702308, 1000)\n",
      "(345913, 1000)\n",
      "(702308, 3)\n",
      "(345913, 3)\n"
     ]
    }
   ],
   "source": [
    "# Check shapes\n",
    "print(X_train_seq.shape)\n",
    "print(X_test_seq.shape)\n",
    "print(y_train_encoded.shape)\n",
    "print(y_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ready-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_cnn_model(dropout_rate, l1_value, l2_value):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(config.vocab_size, config.embedding_dims, input_length = config.maxlen))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Conv1D(config.filters, config.kernel_size, \n",
    "                     kernel_regularizer = regularizers.l1_l2(l1 = l1_value, l2 = l2_value),\n",
    "                     padding = 'valid', activation = 'relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(config.filters, config.kernel_size, padding = 'valid', activation = 'relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(config.hidden_dims, activation = 'relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-board",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the  1 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/10\n",
      "199104/702308 [=======>......................] - ETA: 11:32 - loss: 0.7315 - acc: 0.6244"
     ]
    }
   ],
   "source": [
    "# Define the drop out grid\n",
    "dropout_grid = [0.1, 0.5, 0.9]\n",
    "l1_grid = [2**-5, 2**-6, 2**-7, 2**-8]\n",
    "l2_grid = [2**-5, 2**-6, 2**-7, 2**-8]\n",
    "tot = len(dropout_grid) * len(l1_grid) * len(l2_grid)\n",
    "\n",
    "# Variables for the best result\n",
    "scores = []\n",
    "best_history = [] # place holder\n",
    "best_ind = 0\n",
    "best_acc = 0\n",
    "\n",
    "# Loop through each combination\n",
    "pos = 0\n",
    "for ii in dropout_grid:\n",
    "    for jj in l1_grid:\n",
    "        for kk in l2_grid:\n",
    "            pos = pos + 1\n",
    "            print(\"Fitting the \", pos, \"/\", tot , \" model\")\n",
    "            # define the model\n",
    "            curr_model = define_cnn_model(ii, jj, kk)\n",
    "            \n",
    "            # train the model\n",
    "            curr_history = curr_model.fit(X_train_seq, y_train_encoded, epochs = 10, class_weight = label_weights,\n",
    "                                          batch_size = 64, validation_data = (X_test_seq, y_test_encoded), verbose = 1)\n",
    "            curr_acc = st.mean(curr_history.history['val_acc'][5:10])\n",
    "            \n",
    "            # save the best result\n",
    "            if best_acc < curr_acc:\n",
    "                best_acc = curr_acc\n",
    "                best_ind = pos - 1\n",
    "                best_history = curr_history\n",
    "                \n",
    "print(best_acc)\n",
    "print(best_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-balance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
