{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "romantic-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install keras==2.4.0\n",
    "import tensorflow\n",
    "import keras\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as st\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Activation, Conv1D, Flatten, MaxPooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adopted-level",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wqe3stue) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1803<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/Users/shilpakancharla/Documents/metoo-classification/wandb/run-20210418_135535-wqe3stue/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/Users/shilpakancharla/Documents/metoo-classification/wandb/run-20210418_135535-wqe3stue/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">avid-snowflake-1</strong>: <a href=\"https://wandb.ai/shilpakancharla/metoo-classification/runs/wqe3stue\" target=\"_blank\">https://wandb.ai/shilpakancharla/metoo-classification/runs/wqe3stue</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:wqe3stue). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">worthy-galaxy-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/shilpakancharla/metoo-classification\" target=\"_blank\">https://wandb.ai/shilpakancharla/metoo-classification</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/shilpakancharla/metoo-classification/runs/s61f6r1m\" target=\"_blank\">https://wandb.ai/shilpakancharla/metoo-classification/runs/s61f6r1m</a><br/>\n",
       "                Run data is saved locally in <code>/Users/shilpakancharla/Documents/metoo-classification/wandb/run-20210418_135604-s61f6r1m</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init()\n",
    "config = wandb.config\n",
    "\n",
    "# Set parameters\n",
    "config.vocab_size = 1000\n",
    "config.maxlen = 1000\n",
    "config.batch_size = 32\n",
    "config.embedding_dims = 10\n",
    "config.filters = 16\n",
    "config.kernel_size = 3\n",
    "config.hidden_dims = 250\n",
    "config.epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "corresponding-reservation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2018, 2017, 2019])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('processed_data/clean_data.csv')\n",
    "df.dropna()\n",
    "delete_row = df[df['Years'] == 0].index\n",
    "df = df.drop(delete_row)\n",
    "df['Years'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "alpine-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Tweets with no Stopwords'], df['Years'], test_size = 0.33,\n",
    "                                                   random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "apart-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words = config.vocab_size)\n",
    "tokenizer.fit_on_texts(X_train.astype(str))\n",
    "X_train_tokenized = tokenizer.texts_to_matrix(X_train.astype(str))\n",
    "X_test_tokenized = tokenizer.texts_to_matrix(X_test.astype(str))\n",
    "X_train_seq = sequence.pad_sequences(X_train_tokenized, maxlen = config.maxlen)\n",
    "X_test_seq = sequence.pad_sequences(X_test_tokenized, maxlen = config.maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "legitimate-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "y_train_encoded = LabelBinarizer().fit_transform(y_train)\n",
    "y_test_encoded = LabelBinarizer().fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "intelligent-crawford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.21118492 0.5432297  2.99831792]\n",
      "{0: 1.21118492304933, 1: 0.5432297008596592, 2: 2.9983179213948445}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[2017 2018 2019], y=[2017 2018 2018 ... 2017 2017 2018] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "label_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train.ravel())\n",
    "print(label_weights)\n",
    "label_weights = {i:label_weights[i] for i in range(len(label_weights))} # Create dictionary\n",
    "print(label_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "preceding-neighborhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(702308, 1000)\n",
      "(345913, 1000)\n",
      "(702308, 3)\n",
      "(345913, 3)\n"
     ]
    }
   ],
   "source": [
    "# Check shapes\n",
    "print(X_train_seq.shape)\n",
    "print(X_test_seq.shape)\n",
    "print(y_train_encoded.shape)\n",
    "print(y_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "behind-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 3)\n",
    "\n",
    "def define_cnn_model(dropout_rate, l1_value, l2_value):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(config.vocab_size, config.embedding_dims, input_length = config.maxlen))\n",
    "    model.add(Conv1D(config.filters, config.kernel_size, \n",
    "                     kernel_regularizer = regularizers.l1_l2(l1 = l1_value, l2 = l2_value),\n",
    "                     padding = 'valid', activation = 'relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(config.filters, config.kernel_size, padding = 'valid', activation = 'relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(config.hidden_dims, activation = 'relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-assessment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the  1 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 953s 1ms/step - loss: 0.6446 - acc: 0.6431 - val_loss: 0.6373 - val_acc: 0.6860\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 934s 1ms/step - loss: 0.5710 - acc: 0.6736 - val_loss: 0.6354 - val_acc: 0.6782\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 927s 1ms/step - loss: 0.5486 - acc: 0.6952 - val_loss: 0.6211 - val_acc: 0.6963\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 967s 1ms/step - loss: 0.5290 - acc: 0.7123 - val_loss: 0.6096 - val_acc: 0.7033\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 1119s 2ms/step - loss: 0.5099 - acc: 0.7269 - val_loss: 0.6068 - val_acc: 0.7145\n",
      "Fitting the  2 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 990s 1ms/step - loss: 0.6261 - acc: 0.6496 - val_loss: 0.6030 - val_acc: 0.7151\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 971s 1ms/step - loss: 0.5644 - acc: 0.6779 - val_loss: 0.6086 - val_acc: 0.6917\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 950s 1ms/step - loss: 0.5428 - acc: 0.7005 - val_loss: 0.6343 - val_acc: 0.6715\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 1015s 1ms/step - loss: 0.5230 - acc: 0.7192 - val_loss: 0.5958 - val_acc: 0.7160\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 954s 1ms/step - loss: 0.5048 - acc: 0.7333 - val_loss: 0.5940 - val_acc: 0.7107\n",
      "Fitting the  3 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 972s 1ms/step - loss: 0.6211 - acc: 0.6546 - val_loss: 0.6182 - val_acc: 0.6983\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 955s 1ms/step - loss: 0.5635 - acc: 0.6793 - val_loss: 0.6400 - val_acc: 0.6566\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 957s 1ms/step - loss: 0.5431 - acc: 0.7015 - val_loss: 0.6027 - val_acc: 0.7053\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 969s 1ms/step - loss: 0.5214 - acc: 0.7186 - val_loss: 0.5922 - val_acc: 0.7101\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 961s 1ms/step - loss: 0.5023 - acc: 0.7342 - val_loss: 0.5776 - val_acc: 0.7358\n",
      "Fitting the  4 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 964s 1ms/step - loss: 0.6206 - acc: 0.6546 - val_loss: 0.6674 - val_acc: 0.6215\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 956s 1ms/step - loss: 0.5651 - acc: 0.6786 - val_loss: 0.6221 - val_acc: 0.6831\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 956s 1ms/step - loss: 0.5447 - acc: 0.6997 - val_loss: 0.6058 - val_acc: 0.7057\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 953s 1ms/step - loss: 0.5235 - acc: 0.7187 - val_loss: 0.6220 - val_acc: 0.6922\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 966s 1ms/step - loss: 0.5038 - acc: 0.7330 - val_loss: 0.6708 - val_acc: 0.6334\n",
      "Fitting the  5 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 861s 1ms/step - loss: 0.6139 - acc: 0.6546 - val_loss: 0.6616 - val_acc: 0.6125\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 861s 1ms/step - loss: 0.5615 - acc: 0.6840 - val_loss: 0.6345 - val_acc: 0.6550\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 919s 1ms/step - loss: 0.5397 - acc: 0.7062 - val_loss: 0.6162 - val_acc: 0.6890\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 954s 1ms/step - loss: 0.5188 - acc: 0.7234 - val_loss: 0.6273 - val_acc: 0.6740\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 958s 1ms/step - loss: 0.4982 - acc: 0.7371 - val_loss: 0.5962 - val_acc: 0.7159\n",
      "Fitting the  6 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 961s 1ms/step - loss: 0.6108 - acc: 0.6558 - val_loss: 0.6197 - val_acc: 0.6718\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 957s 1ms/step - loss: 0.5629 - acc: 0.6821 - val_loss: 0.6235 - val_acc: 0.6752\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 861s 1ms/step - loss: 0.5418 - acc: 0.7024 - val_loss: 0.6184 - val_acc: 0.6919\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 860s 1ms/step - loss: 0.5209 - acc: 0.7197 - val_loss: 0.5994 - val_acc: 0.7075\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 855s 1ms/step - loss: 0.4985 - acc: 0.7350 - val_loss: 0.5947 - val_acc: 0.7242\n",
      "Fitting the  7 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 857s 1ms/step - loss: 0.6140 - acc: 0.6512 - val_loss: 0.6150 - val_acc: 0.6976\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 850s 1ms/step - loss: 0.5593 - acc: 0.6828 - val_loss: 0.6123 - val_acc: 0.7078\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 845s 1ms/step - loss: 0.5352 - acc: 0.7083 - val_loss: 0.6203 - val_acc: 0.6873\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 848s 1ms/step - loss: 0.5136 - acc: 0.7269 - val_loss: 0.6039 - val_acc: 0.7166\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 850s 1ms/step - loss: 0.4944 - acc: 0.7407 - val_loss: 0.6393 - val_acc: 0.6925\n",
      "Fitting the  8 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 853s 1ms/step - loss: 0.6086 - acc: 0.6567 - val_loss: 0.6437 - val_acc: 0.6569\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 851s 1ms/step - loss: 0.5627 - acc: 0.6799 - val_loss: 0.5751 - val_acc: 0.7483\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 851s 1ms/step - loss: 0.5406 - acc: 0.7013 - val_loss: 0.6548 - val_acc: 0.6530\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 857s 1ms/step - loss: 0.5201 - acc: 0.7201 - val_loss: 0.6174 - val_acc: 0.6888\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 860s 1ms/step - loss: 0.5002 - acc: 0.7344 - val_loss: 0.6135 - val_acc: 0.7021\n",
      "Epoch 00005: early stopping\n",
      "Fitting the  9 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 847s 1ms/step - loss: 0.6058 - acc: 0.6537 - val_loss: 0.6244 - val_acc: 0.6903\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 844s 1ms/step - loss: 0.5582 - acc: 0.6826 - val_loss: 0.6024 - val_acc: 0.7110\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 844s 1ms/step - loss: 0.5340 - acc: 0.7073 - val_loss: 0.6155 - val_acc: 0.6907\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 847s 1ms/step - loss: 0.5107 - acc: 0.7273 - val_loss: 0.5900 - val_acc: 0.7111\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 844s 1ms/step - loss: 0.4904 - acc: 0.7421 - val_loss: 0.6036 - val_acc: 0.7090\n",
      "Fitting the  10 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 851s 1ms/step - loss: 0.6039 - acc: 0.6551 - val_loss: 0.6577 - val_acc: 0.6251\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 857s 1ms/step - loss: 0.5594 - acc: 0.6803 - val_loss: 0.6484 - val_acc: 0.6241\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 850s 1ms/step - loss: 0.5379 - acc: 0.7026 - val_loss: 0.6132 - val_acc: 0.6927\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 850s 1ms/step - loss: 0.5166 - acc: 0.7207 - val_loss: 0.6227 - val_acc: 0.6792\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 858s 1ms/step - loss: 0.4977 - acc: 0.7343 - val_loss: 0.6359 - val_acc: 0.6701\n",
      "Fitting the  11 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 861s 1ms/step - loss: 0.6032 - acc: 0.6552 - val_loss: 0.6194 - val_acc: 0.6884\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 855s 1ms/step - loss: 0.5601 - acc: 0.6811 - val_loss: 0.6299 - val_acc: 0.6732\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702308/702308 [==============================] - 859s 1ms/step - loss: 0.5370 - acc: 0.7050 - val_loss: 0.6028 - val_acc: 0.7027\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 857s 1ms/step - loss: 0.5150 - acc: 0.7231 - val_loss: 0.6120 - val_acc: 0.7057\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 854s 1ms/step - loss: 0.4947 - acc: 0.7373 - val_loss: 0.5940 - val_acc: 0.7076\n",
      "Fitting the  12 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 922s 1ms/step - loss: 0.6037 - acc: 0.6556 - val_loss: 0.6421 - val_acc: 0.6373\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 920s 1ms/step - loss: 0.5598 - acc: 0.6834 - val_loss: 0.6442 - val_acc: 0.6377\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 956s 1ms/step - loss: 0.5361 - acc: 0.7061 - val_loss: 0.5793 - val_acc: 0.7350\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 952s 1ms/step - loss: 0.5130 - acc: 0.7246 - val_loss: 0.6122 - val_acc: 0.6879\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 952s 1ms/step - loss: 0.4913 - acc: 0.7391 - val_loss: 0.5559 - val_acc: 0.7569\n",
      "Fitting the  13 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 959s 1ms/step - loss: 0.6031 - acc: 0.6565 - val_loss: 0.6276 - val_acc: 0.6650\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 953s 1ms/step - loss: 0.5573 - acc: 0.6846 - val_loss: 0.6121 - val_acc: 0.7014\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 953s 1ms/step - loss: 0.5329 - acc: 0.7080 - val_loss: 0.5954 - val_acc: 0.7180\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 963s 1ms/step - loss: 0.5105 - acc: 0.7268 - val_loss: 0.6270 - val_acc: 0.6830\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 954s 1ms/step - loss: 0.4907 - acc: 0.7413 - val_loss: 0.6108 - val_acc: 0.6928\n",
      "Fitting the  14 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 1282s 2ms/step - loss: 0.5981 - acc: 0.6555 - val_loss: 0.6076 - val_acc: 0.6925\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 1077s 2ms/step - loss: 0.5563 - acc: 0.6842 - val_loss: 0.6451 - val_acc: 0.6570\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 1099s 2ms/step - loss: 0.5317 - acc: 0.7097 - val_loss: 0.6109 - val_acc: 0.6957\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 1046s 1ms/step - loss: 0.5088 - acc: 0.7284 - val_loss: 0.5742 - val_acc: 0.7342\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 1000s 1ms/step - loss: 0.4877 - acc: 0.7430 - val_loss: 0.6100 - val_acc: 0.7080\n",
      "Fitting the  15 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 956s 1ms/step - loss: 0.6014 - acc: 0.6573 - val_loss: 0.6160 - val_acc: 0.6956\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 1215s 2ms/step - loss: 0.5572 - acc: 0.6830 - val_loss: 0.6123 - val_acc: 0.6993\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 1180s 2ms/step - loss: 0.5326 - acc: 0.7064 - val_loss: 0.5942 - val_acc: 0.7143\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 961s 1ms/step - loss: 0.5102 - acc: 0.7247 - val_loss: 0.5880 - val_acc: 0.7240\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 1330s 2ms/step - loss: 0.4894 - acc: 0.7393 - val_loss: 0.5998 - val_acc: 0.7178\n",
      "Fitting the  16 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 1412s 2ms/step - loss: 0.5994 - acc: 0.6564 - val_loss: 0.6122 - val_acc: 0.6910\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 1304s 2ms/step - loss: 0.5557 - acc: 0.6849 - val_loss: 0.6594 - val_acc: 0.6400\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 1130s 2ms/step - loss: 0.5310 - acc: 0.7113 - val_loss: 0.6260 - val_acc: 0.6811\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 1056s 2ms/step - loss: 0.5082 - acc: 0.7301 - val_loss: 0.6492 - val_acc: 0.6579\n",
      "Epoch 00004: early stopping\n",
      "Fitting the  17 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 974s 1ms/step - loss: 0.6379 - acc: 0.6454 - val_loss: 0.6521 - val_acc: 0.6179\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 993s 1ms/step - loss: 0.5797 - acc: 0.6663 - val_loss: 0.6488 - val_acc: 0.6439\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 968s 1ms/step - loss: 0.5698 - acc: 0.6825 - val_loss: 0.6354 - val_acc: 0.6738\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 966s 1ms/step - loss: 0.5630 - acc: 0.6926 - val_loss: 0.5962 - val_acc: 0.7245\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 979s 1ms/step - loss: 0.5566 - acc: 0.7004 - val_loss: 0.6242 - val_acc: 0.6872\n",
      "Fitting the  18 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 987s 1ms/step - loss: 0.6500 - acc: 0.6372 - val_loss: 0.6745 - val_acc: 0.6335\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 1033s 1ms/step - loss: 0.6044 - acc: 0.6519 - val_loss: 0.6542 - val_acc: 0.6808\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 962s 1ms/step - loss: 0.5973 - acc: 0.6620 - val_loss: 0.6421 - val_acc: 0.6811\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 982s 1ms/step - loss: 0.5933 - acc: 0.6734 - val_loss: 0.6417 - val_acc: 0.6867\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 971s 1ms/step - loss: 0.5877 - acc: 0.6795 - val_loss: 0.6628 - val_acc: 0.6460\n",
      "Fitting the  19 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 1020s 1ms/step - loss: 0.6321 - acc: 0.6493 - val_loss: 0.6792 - val_acc: 0.5994\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 983s 1ms/step - loss: 0.5809 - acc: 0.6662 - val_loss: 0.6200 - val_acc: 0.6779\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 1022s 1ms/step - loss: 0.5715 - acc: 0.6804 - val_loss: 0.6146 - val_acc: 0.6973\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 1002s 1ms/step - loss: 0.5640 - acc: 0.6888 - val_loss: 0.6005 - val_acc: 0.7184\n",
      "Epoch 5/5\n",
      "702308/702308 [==============================] - 1275s 2ms/step - loss: 0.5588 - acc: 0.6974 - val_loss: 0.6012 - val_acc: 0.7255\n",
      "Fitting the  20 / 48  model\n",
      "Train on 702308 samples, validate on 345913 samples\n",
      "Epoch 1/5\n",
      "702308/702308 [==============================] - 1373s 2ms/step - loss: 0.6279 - acc: 0.6488 - val_loss: 0.6494 - val_acc: 0.6696\n",
      "Epoch 2/5\n",
      "702308/702308 [==============================] - 1292s 2ms/step - loss: 0.5781 - acc: 0.6689 - val_loss: 0.6185 - val_acc: 0.6993\n",
      "Epoch 3/5\n",
      "702308/702308 [==============================] - 972s 1ms/step - loss: 0.5692 - acc: 0.6805 - val_loss: 0.6049 - val_acc: 0.6970\n",
      "Epoch 4/5\n",
      "702308/702308 [==============================] - 977s 1ms/step - loss: 0.5620 - acc: 0.6914 - val_loss: 0.6261 - val_acc: 0.6885\n",
      "Epoch 5/5\n",
      "221760/702308 [========>.....................] - ETA: 10:14 - loss: 0.5524 - acc: 0.7019"
     ]
    }
   ],
   "source": [
    "# Define the drop out grid\n",
    "dropout_grid = [0.1, 0.5, 0.9]\n",
    "l1_grid = [2**-5, 2**-6, 2**-7, 2**-8]\n",
    "l2_grid = [2**-5, 2**-6, 2**-7, 2**-8]\n",
    "tot = len(dropout_grid) * len(l1_grid) * len(l2_grid)\n",
    "\n",
    "# Variables for the best result\n",
    "best_history = [] # place holder\n",
    "best_ind = 0\n",
    "best_acc = 0\n",
    "\n",
    "# Loop through each combination\n",
    "pos = 0\n",
    "for ii in dropout_grid:\n",
    "    for jj in l1_grid:\n",
    "        for kk in l2_grid:\n",
    "            pos = pos + 1\n",
    "            print(\"Fitting the \", pos, \"/\", tot , \" model\")\n",
    "            # define the model\n",
    "            curr_model = define_cnn_model(ii, jj, kk)\n",
    "            \n",
    "            # train the model\n",
    "            curr_history = curr_model.fit(X_train_seq, y_train_encoded, epochs = 5, class_weight = label_weights,\n",
    "                                          batch_size = 64, validation_data = (X_test_seq, y_test_encoded), \n",
    "                                          callbacks = [callback], verbose = 1)\n",
    "            curr_acc = st.mean(curr_history.history['val_acc'])\n",
    "            \n",
    "            # save the best result\n",
    "            if best_acc < curr_acc:\n",
    "                best_acc = curr_acc\n",
    "                best_ind = pos - 1\n",
    "                best_history = curr_history\n",
    "                \n",
    "print(best_acc)\n",
    "print(best_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-disaster",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
