{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "tropical-organizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shilp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "import re\n",
    "import nltk\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "warnings.filterwarnings('ignore')\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Import custom functions\n",
    "from modify_df import *\n",
    "from custom_regex import *\n",
    "from sentence_processing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-feedback",
   "metadata": {},
   "source": [
    "# Merging Datasets, Data Cleaning\n",
    "\n",
    "There are currently four different datasets we are using (along with some of their columns):\n",
    "\n",
    "| Oct 2017  | Nov 2017 - Dec 2017 | Sept 2018 - Feb 2019 | Oct 2019 |\n",
    "| --- | --- | --- | --- |\n",
    "| 350K | 390K | 695K | 15K |\n",
    "| id, date of tweet, text | text, favorited, created | text, location, created | text, language, created at |\n",
    "\n",
    "We want the date of the tweet and the text content of the tweet itself. We want to focus on tweets are in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "shared-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datasets\n",
    "df_oct17 = pd.read_csv('raw_data/oct2017.csv')\n",
    "df_novdec17 = pd.read_csv('raw_data/novdec17.csv')\n",
    "df_sept18feb19 = pd.read_csv('raw_data/sept2018feb2019.csv')\n",
    "df_oct19 = pd.read_csv('raw_data/oct2019.csv')\n",
    "\n",
    "# Drop columns from each dataframe as necessary\n",
    "df_oct17 = df_oct17.drop(columns = ['id', 'insertdate', 'twitterhandle', 'followers', 'hashtagsearched',\n",
    "                                   'tweetid', 'lastcontactdate', 'lasttimelinepull', 'lasttimetweetsanalyzed',\n",
    "                                   'numberoftweetsanalysed', 'numberoftweetsabouthash', 'actualtwitterdate'])\n",
    "df_oct17 = df_oct17.loc[:, ~df_oct17.columns.str.contains('^Unnamed')]\n",
    "df_oct17 = df_oct17.dropna()\n",
    "df_oct17 = df_oct17.reset_index()\n",
    "df_novdec17 = df_novdec17.drop(columns = ['favorited', 'favoriteCount', 'replyToSN', 'truncated', 'replyToSID',\n",
    "                                         'id', 'replyToUID', 'statusSource', 'screenName', 'retweetCount',\n",
    "                                         'isRetweet', 'retweeted', 'longitude', 'latitude'])\n",
    "df_novdec17 = df_novdec17.loc[:, ~df_novdec17.columns.str.contains('^Unnamed')]\n",
    "df_novdec17 = df_novdec17.dropna()\n",
    "df_novdec17 = df_novdec17.reset_index()\n",
    "df_sept18feb19 = df_sept18feb19.drop(columns = ['status_id', 'favorite_count', 'retweet_count', 'location',\n",
    "                                                'followers_count', 'friends_count', 'statuses_count', 'category'])\n",
    "df_sept18feb19 = df_sept18feb19.dropna()\n",
    "df_sept18feb19 = df_sept18feb19.reset_index()\n",
    "df_oct19 = df_oct19.drop(columns = ['Id', 'Lenght', 'Source', 'Favorite_count', 'Retweet_count'])\n",
    "df_oct19 = df_oct19[df_oct19['Lang'] == 'en']\n",
    "df_oct19 = df_oct19.drop(columns = ['Lang'])\n",
    "df_oct19 = df_oct19.dropna()\n",
    "df_oct19 = df_oct19.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cellular-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oct17 = get_oct17_data(df_oct17, 'dateoftweet')\n",
    "df_novdec17 = get_novdec17_data(df_novdec17, 'created')\n",
    "df_sept18feb19 = get_sept18feb19_data(df_sept18feb19, 'created_at')\n",
    "df_oct19 = get_oct19_data(df_oct19, 'Created_at')\n",
    "\n",
    "# Concatenate all frames\n",
    "data = pd.concat([df_oct17, df_novdec17, df_sept18feb19, df_oct19])\n",
    "data = data.reset_index()\n",
    "data = data.drop(columns = ['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-disco",
   "metadata": {},
   "source": [
    "We perform the following operations on the 'Text' column of the dataframe:\n",
    "* lowercase\n",
    "* duplicates (keeping duplicate tweets could lead to bias)\n",
    "* retweets\n",
    "* hyperlinks\n",
    "* emojis\n",
    "* mentions\n",
    "* length greater than 280\n",
    "* whitespaces\n",
    "\n",
    "We also ensure there are no missing values in our dataframe at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d511f195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1606028"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c9a972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1606028"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting tweets to lowercase\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.lower() if type(x) == str else x)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9bcdadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048258"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing duplicates\n",
    "data_nodup = data.drop_duplicates(subset = 'Tweet', keep = 'first')\n",
    "data_nodup = data_nodup.reset_index()\n",
    "data_nodup = data_nodup.drop(columns = ['index'])\n",
    "len(data_nodup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b0b05f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048258"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing 'RT'\n",
    "data['Tweet'] = data_nodup['Tweet'].apply(lambda x: re.sub(r'http://t(?!$)', '', x) if type(x) == str else x)\n",
    "data_nodup['Tweet'] = data_nodup['Tweet'].apply(lambda x: x.replace('rt ', '') if type(x) == str else x)\n",
    "data_nodup['Tweet'] = data_nodup['Tweet'].apply(lambda x: x.replace('rt', '') if type(x) == str else x)\n",
    "len(data_nodup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf77c615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048258"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing emojis\n",
    "data_nodup['Tweet'] = data_nodup['Tweet'].apply(lambda x: de_emojify(x) if type(x) == str else x)\n",
    "len(data_nodup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdd94c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048258"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete URLs\n",
    "data_nodup['Tweet'] = data_nodup['Tweet'].apply(lambda x: re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE) if type(x) == str else x)\n",
    "len(data_nodup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2744851b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048258"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove mentions\n",
    "data_nodup['Tweet'] = np.vectorize(remove_regex)(data_nodup['Tweet'], \"@[\\w]*\")\n",
    "len(data_nodup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98062a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048258"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove special characters (except hashtags and apostrophes), replace with whitespace\n",
    "data_nodup['Tweet'] = data_nodup['Tweet'].str.replace(\"[^a-zA-Z#']\", \" \")\n",
    "len(data_nodup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8025efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048258"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove single hashtags with nothing following them\n",
    "data_nodup['Tweet'] = np.vectorize(remove_regex)(data_nodup['Tweet'], \" # \")\n",
    "len(data_nodup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f098032a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048258"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove trailing whitespace\n",
    "data_nodup['Tweet'] = data_nodup.apply(lambda x: x.strip() if type(x) == str else x)\n",
    "len(data_nodup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "patent-royalty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tweet           0\n",
       "Years           0\n",
       "Tweet Length    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure no missing values\n",
    "data_nodup.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b90d8bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Years</th>\n",
       "      <th>Tweet Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cuando esta se ora habla es como leer los...</td>\n",
       "      <td>2018</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>will require institutions that receive gra...</td>\n",
       "      <td>2018</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>listening to the awesome feminist scholar cynt...</td>\n",
       "      <td>2018</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>...</td>\n",
       "      <td>2018</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a ver  donde est n todas las voceras colomb...</td>\n",
       "      <td>2018</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>we cant romanticize the same things we rally...</td>\n",
       "      <td>2018</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>db is a new initiative by a group of german...</td>\n",
       "      <td>2018</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>very proud to become a white ribbon uk champ...</td>\n",
       "      <td>2018</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#metoo movement lawmaker investigated for sexu...</td>\n",
       "      <td>2018</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#geraldbutts is #justintrudeau  amp  #liber...</td>\n",
       "      <td>2018</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>couldnt be more proud of   i know this diffi...</td>\n",
       "      <td>2018</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>as paof our focus on democracy in our #shift...</td>\n",
       "      <td>2018</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>missed our quaerly hr webinar  our guru adrian...</td>\n",
       "      <td>2018</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>presidents  posts and liberal pandering to p...</td>\n",
       "      <td>2018</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gotta love the hypocrisy of a woman who margi...</td>\n",
       "      <td>2018</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweet Years  Tweet Length\n",
       "0        cuando esta se ora habla es como leer los...  2018            83\n",
       "1       will require institutions that receive gra...  2018           123\n",
       "2   listening to the awesome feminist scholar cynt...  2018           140\n",
       "3                                                 ...  2018            84\n",
       "4      a ver  donde est n todas las voceras colomb...  2018           121\n",
       "5     we cant romanticize the same things we rally...  2018           120\n",
       "6      db is a new initiative by a group of german...  2018           121\n",
       "7     very proud to become a white ribbon uk champ...  2018           124\n",
       "8   #metoo movement lawmaker investigated for sexu...  2018           116\n",
       "9      #geraldbutts is #justintrudeau  amp  #liber...  2018           107\n",
       "10    couldnt be more proud of   i know this diffi...  2018           110\n",
       "11    as paof our focus on democracy in our #shift...  2018           120\n",
       "12  missed our quaerly hr webinar  our guru adrian...  2018           138\n",
       "13    presidents  posts and liberal pandering to p...  2018            99\n",
       "14   gotta love the hypocrisy of a woman who margi...  2018           124"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nodup.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-portrait",
   "metadata": {},
   "source": [
    "# Lemmatization of Parts of Speech\n",
    "Lemmatizing a part of speech means that we classify each word as an adjective, adverb, noun, or verb. Each word in the sentence is treated as a token and a tag is given vased off the lexical database Wordnet\n",
    "(https://wordnet.princeton.edu/). Tuples of tokens and wordnet tags are then crated, and we look for a match. If there is a match present, the word is classified (lemmatized) as one of the parts of speech. One exception exists ('ass'), which has been tweaked using the `get_lemma` function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply get_lemma function\n",
    "data_nodup['Lemmatized'] = data_nodup['Tweet'].apply(lambda x: get_lemma(x))\n",
    "\n",
    "# Removing spaces after hashtags\n",
    "data_nodup['Lemmatized'] = data_nodup['Lemmatized'].str.replace('# ', '#')\n",
    "\n",
    "# Removing spaces after apostrophes\n",
    "data_nodup['Lemmatized'] = data_nodup['Lemmatized'].str.replace(\" '\", \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8775cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_nodup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "korean-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stopwords from English\n",
    "stop = stopwords.words('english')\n",
    "data_nodup['Tweets with no Stopwords'] = data_nodup['Lemmatized'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cross-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing words shorter than two characters because they will likely not be relevant\n",
    "data_nodup['Short Tweets'] = data_nodup['Tweets with no Stopwords'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "czech-environment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2017' '2018' '2019']\n"
     ]
    }
   ],
   "source": [
    "print(pd.unique(data_nodup['Years']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "retained-necessity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tweet                       0\n",
       "Years                       0\n",
       "Tweet Length                0\n",
       "Lemmatized                  0\n",
       "Tweets with no Stopwords    0\n",
       "Short Tweets                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nodup.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "formed-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "data_nodup.to_csv(r'processed_data/clean_data.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
