{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tropical-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import re\n",
    "import nltk\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "warnings.filterwarnings('ignore')\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Import custom functions\n",
    "from modify_df import *\n",
    "from custom_regex import *\n",
    "from sentence_processing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-feedback",
   "metadata": {},
   "source": [
    "# Merging Datasets, Data Cleaning\n",
    "\n",
    "There are currently four different datasets we are using (along with some of their columns):\n",
    "\n",
    "| Oct 2017  | Nov 2017 - Dec 2017 | Sept 2018 - Feb 2019 | Oct 2019 |\n",
    "| --- | --- | --- | --- |\n",
    "| 350K | 390K | 695K | 15K |\n",
    "| id, date of tweet, text | text, favorited, created | text, location, created | text, language, created at |\n",
    "\n",
    "We want the date of the tweet and the text content of the tweet itself. We want to focus on tweets are in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "shared-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datasets\n",
    "df_oct17 = pd.read_csv('raw_data/oct2017.csv')\n",
    "df_novdec17 = pd.read_csv('raw_data/novdec17.csv')\n",
    "df_sept18feb19 = pd.read_csv('raw_data/sept2018feb2019.csv')\n",
    "df_oct19 = pd.read_csv('raw_data/oct2019.csv')\n",
    "\n",
    "# Drop columns from each dataframe as necessary\n",
    "df_oct17 = df_oct17.drop(columns = ['id', 'insertdate', 'twitterhandle', 'followers', 'hashtagsearched',\n",
    "                                   'tweetid', 'lastcontactdate', 'lasttimelinepull', 'lasttimetweetsanalyzed',\n",
    "                                   'numberoftweetsanalysed', 'numberoftweetsabouthash', 'actualtwitterdate'])\n",
    "df_oct17 = df_oct17.loc[:, ~df_oct17.columns.str.contains('^Unnamed')]\n",
    "df_oct17 = df_oct17.dropna()\n",
    "df_oct17 = df_oct17.reset_index()\n",
    "df_novdec17 = df_novdec17.drop(columns = ['favorited', 'favoriteCount', 'replyToSN', 'truncated', 'replyToSID',\n",
    "                                         'id', 'replyToUID', 'statusSource', 'screenName', 'retweetCount',\n",
    "                                         'isRetweet', 'retweeted', 'longitude', 'latitude'])\n",
    "df_novdec17 = df_novdec17.loc[:, ~df_novdec17.columns.str.contains('^Unnamed')]\n",
    "df_novdec17 = df_novdec17.dropna()\n",
    "df_novdec17 = df_novdec17.reset_index()\n",
    "df_sept18feb19 = df_sept18feb19.drop(columns = ['status_id', 'favorite_count', 'retweet_count', 'location',\n",
    "                                                'followers_count', 'friends_count', 'statuses_count', 'category'])\n",
    "df_sept18feb19 = df_sept18feb19.dropna()\n",
    "df_sept18feb19 = df_sept18feb19.reset_index()\n",
    "df_oct19 = df_oct19.drop(columns = ['Id', 'Lenght', 'Source', 'Favorite_count', 'Retweet_count'])\n",
    "df_oct19 = df_oct19[df_oct19['Lang'] == 'en']\n",
    "df_oct19 = df_oct19.drop(columns = ['Lang'])\n",
    "df_oct19 = df_oct19.dropna()\n",
    "df_oct19 = df_oct19.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cellular-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oct17 = get_oct17_data(df_oct17, 'dateoftweet')\n",
    "df_novdec17 = get_novdec17_data(df_novdec17, 'created')\n",
    "df_sept18feb19 = get_sept18feb19_data(df_sept18feb19, 'created_at')\n",
    "df_oct19 = get_oct19_data(df_oct19, 'Created_at')\n",
    "\n",
    "# Concatenate all frames\n",
    "data = pd.concat([df_oct17, df_novdec17, df_sept18feb19, df_oct19])\n",
    "data = data.reset_index()\n",
    "data = data.drop(columns = ['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-disco",
   "metadata": {},
   "source": [
    "We perform the following operations on the 'Text' column of the dataframe:\n",
    "* lowercase\n",
    "* duplicates (keeping duplicate tweets could lead to bias)\n",
    "* retweets\n",
    "* hyperlinks\n",
    "* emojis\n",
    "* mentions\n",
    "* length greater than 280\n",
    "* whitespaces\n",
    "\n",
    "We also ensure there are no missing values in our dataframe at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "patent-royalty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tweet           0\n",
       "Years           0\n",
       "Tweet Length    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting tweets to lowercase\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.lower() if type(x) == str else x)\n",
    "\n",
    "# Removing duplicates\n",
    "data = data.drop_duplicates(subset = 'Tweet', keep = 'first')\n",
    "data = data.reset_index()\n",
    "data = data.drop(columns = ['index'])\n",
    "\n",
    "# Removing 'RT'\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(r'http://t(?!$)', '', x) if type(x) == str else x)\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.replace('rt ', '') if type(x) == str else x)\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.replace('rt', '') if type(x) == str else x)\n",
    "\n",
    "# Removing emojis\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: de_emojify(x) if type(x) == str else x)\n",
    "\n",
    "# Delete URLs\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE) if type(x) == str else x)\n",
    "\n",
    "# Remove mentions\n",
    "data['Tweet'] = np.vectorize(remove_regex)(data['Tweet'], \"@[\\w]*\")\n",
    "\n",
    "# Remove special characters (except hashtags and apostrophes), replace with whitespace\n",
    "data['Tweet'] = data['Tweet'].str.replace(\"[^a-zA-Z#']\", \" \")\n",
    "\n",
    "# Remove single hashtags with nothing following them\n",
    "data['Tweet'] = np.vectorize(remove_regex)(data['Tweet'], \" # \")\n",
    "\n",
    "# Check that there are no tweets greater than 280 characters\n",
    "data['Tweet Length'] = data['Tweet'].apply(lambda x: len(x))\n",
    "data = data.loc[data['Tweet Length'] > 280]\n",
    "data.sort_values(by = ['Tweet Length'], ascending = False)\n",
    "\n",
    "# Ensure no missing values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-portrait",
   "metadata": {},
   "source": [
    "# Lemmatization of Parts of Speech\n",
    "Lemmatizing a part of speech means that we classify each word as an adjective, adverb, noun, or verb. Each word in the sentence is treated as a token and a tag is given vased off the lexical database Wordnet\n",
    "(https://wordnet.princeton.edu/). Tuples of tokens and wordnet tags are then crated, and we look for a match. If there is a match present, the word is classified (lemmatized) as one of the parts of speech. One exception exists ('ass'), which has been tweaked using the `get_lemma` function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rural-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply get_lemma function\n",
    "data['Lemmatized'] = data['Tweet'].apply(lambda x: get_lemma(x))\n",
    "\n",
    "# Removing spaces after hashtags\n",
    "data['Lemmatized'] = data['Lemmatized'].str.replace('# ', '#')\n",
    "\n",
    "# Removing spaces after apostrophes\n",
    "data['Lemmatized'] = data['Lemmatized'].str.replace(\" '\", \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "korean-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stopwords from English\n",
    "stop = stopwords.words('english')\n",
    "data['Tweets with no Stopwords'] = data['Lemmatized'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cross-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing words shorter than two characters because they will likely not be relevant\n",
    "data['Short Tweets'] = data['Tweets with no Stopwords'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "czech-environment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2017' '2018' '2019']\n"
     ]
    }
   ],
   "source": [
    "print(pd.unique(data['Years']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "retained-necessity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tweet                       0\n",
       "Years                       0\n",
       "Tweet Length                0\n",
       "Lemmatized                  0\n",
       "Tweets with no Stopwords    0\n",
       "Short Tweets                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "formed-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "data.to_csv(r'processed_data/clean_data.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
